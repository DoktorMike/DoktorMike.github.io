---
title: "A few thoughts on apparent bimodality for regression problems!"
author: "Dr. Michael Green"
date: "Apr 9, 2017"
output: html_document
layout: post
published: false
status: process
use_math: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(rstan)
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(dautility)
library(damodel)
library(nloptr)
rstan_options(auto_write = TRUE)
```

# Motivation

Did you ever run into a scenario when your data is showing two distinctive relationships but you're trying to solve for it with one regression line? This happens to me a lot. So I thought about having some fun with it intead of dreading it and the nasty consequences that may arise from this behaviour. Below you'll see a plot featuring two variables, $x$, and $y$ where we are tasked with figuring out how the value of $y$ depends on $x$.

```{r problemplot}
mydf<-tibble(x=seq(0,30,0.2), z=ifelse(runif(1:length(x))>0.5, 1, 2), y=x*ifelse(z<2, 1, 3)+rnorm(length(x), 0, 5))
# mydf<-tibble(x=seq(0,30,0.2), y=x*ifelse(runif(1:length(x))>0.5, 1, 3)+rnorm(length(x), 0, 5))
ggplot(mydf, aes(y=y, x=x)) + geom_point() + theme_minimal()
```

Naturally, what comes to most peoples mind is that we need to model $y_t=\omega f(x_t)+\epsilon$ where $f$ and $\omega$ are currently unknown. The most straightforward solution to this is to assume that we are in a linear regime and consequently that $f(x)=I(x)=x$ where $I$ is the identity function. The equation then quickly becomes $y_t=\omega x_t+\epsilon$ at which time data scientists usually rejoice and apply linear regression. So let's do just that shall we.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
mylm<-lm(y~x, data=mydf)
p1<-ggplot(mydf, aes(y=y, x=x)) + geom_point() + theme_minimal() + geom_smooth(method="lm")
p2<-ggplot(mydf, aes(y=y, x=x, group=z, color=factor(z))) + geom_point() + theme_minimal() + 
  geom_smooth(method="lm") + theme(legend.position = "none")
multiplot(p1, p2, cols = 2)
```

Most of us would agree that the solution with the linear model to the left is not a very nice scenario. We're always off in terms of knowing the real $E[y|x]$. Conceptually this is not very difficult though. We humans do this all the time. If I show you another solution which looks like the one to the right then what would you say? Hopefully you would recognise this as something you would approve of. The problem with this is that a linear model cannot capture this. You need a transformation function to accomplish this. 

```{r linearregression, message=FALSE, warning=FALSE, include=FALSE, cache=TRUE}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
modelstring<-"
data {
  int N;
  real x[N];
  real y[N];
}
parameters {
  real alpha;
  real beta;
  real sigma;
}
model {
  alpha ~ normal(0, 1);
  beta ~ cauchy(0,10);
  for(n in 1:N)
    y[n] ~ normal(beta*x[n]+alpha, sigma);
}
generated quantities {
  real y_pred[N];
  for (n in 1:N)
    y_pred[n] = x[n]*beta+alpha;
}
"
sfit<-stan(model_code = modelstring, data = list(N=nrow(mydf), x=mydf$x, y=mydf$y))
```

But wait! We're all Bayesians here aren't we? So maybe we can caputure this behavior by just letting our model support two modes for the slope parameter? As such we would never really know which slope cluster that would be chosen at any given time and naturally the expectation would end up between the both lines where the posterior probability is zero. Let's have a look at what the following model does when exposed to this data.

$$ \begin{align}
y_t &\sim \mathcal N(\mu_t, \sigma)\\
\mu_t &=\beta x_t + \alpha\\ 
\beta &\sim \mathcal C(0, 10)\\
\alpha &\sim \mathcal N(0, 1)\\
\sigma &\sim \mathcal U(0.01, \inf) 
\end{align} $$

Below you can see the plotted simulated regression lines from the model. Not great is it? Not only did our assumption of bimodality fall through but we're indeed no better of than before. Why? Well, in this case the mathematical formulation of the problem was just plain wrong. Depending on multimodality to cover up for your model specification sins is just bad practice. 

```{r prediction plot, echo=FALSE, message=FALSE, warning=FALSE}
plotFit<-function(sfitdf, mydf, n=3){
  mynames<-grep("y_pred", colnames(sfitdf), value = T)
  tmpdf<-gather(data.frame(x=mydf$x, y=mydf$y, 
                           t(sfitdf[sample(1:nrow(sfitdf), n), mynames])), 
                "Measurement", "Value", -x)
  ggplot(tmpdf, aes(y=Value, x=x, color=Measurement)) + geom_point()
}
sfitdf<-as.data.frame(sfit)
#qplot(sfitdf$beta)
preddf<-tibble(x=mydf$x, y=mydf$y, yhat=colMeans(sfitdf[, grep("y_pred", colnames(sfitdf), value=TRUE)]))
# ggplot(gather(preddf, key, value, -x), aes(y=value, x=x, color=key)) + geom_point() + theme_minimal()
plotFit(sfitdf, mydf, 10) + theme_minimal() + scale_color_discrete() + xlab("x") + 
  ylab("y") + theme(legend.position = "none")
```

Ok, so if the previous model was badly specified then what should we do to fix it? In principle we want the following behavior $y_t=x_t(\beta+\omega z_t)+\alpha$ where $z_t$ is a binary state variable indicating whether the current $x_t$ has the first or the second response type. The full model we then might want to consider looks like this.

$$ \begin{align}
y_t &\sim \mathcal N(\mu_t, \sigma)\\
\mu_t &=x_t(\beta+\omega z_t)+\alpha\\
\omega &\sim \mathcal N(0, 1)\\
z_t &\sim \mathcal{Bin}(1, 0.5)\\
\beta &\sim \mathcal C(0, 10)\\
\alpha &\sim \mathcal N(0, 1)\\
\sigma &\sim \mathcal U(0.01, \inf) 
\end{align} $$

This would allow the state to be modeled as a latent variable in time. This is very useful for a variety of problems where we know something to be true but lack observed data to quantify it. However, modeling discrete latent variables can be computationally demanding if all you are really looking for is an extra dimension. We can of course design this. So instead of viewing $z_t$ as a latent state variable we can actually precode the state by unsupervised hierarchical clustering. The code in R would look like this.

```{r Adding clustered state, echo=TRUE, message=FALSE, warning=FALSE}
mydf<-mutate(mydf, zz=cutree(hclust(dist(mydf[, c("y", "x")])), 2))
```

which encodes the clustered state in a variable called $zz$. Consequently it would produce a hierarchical cluster like the one below.

```{r clusterplot, echo=FALSE, message=FALSE, warning=FALSE}
plot(hclust(dist(mydf[, c("y", "x")])))
```

This leaves us in a position where we can treat $z_t$ as observed data even though we sort of clustered it. The revised math is given below. 

$$ \begin{align}
y_t &\sim \mathcal N(\mu_t, \sigma)\\
\mu_t &=x_t(\beta+\omega z_t)+\alpha\\
\omega &\sim \mathcal N(0, 1)\\
\beta &\sim \mathcal C(0, 10)\\
\alpha &\sim \mathcal N(0, 1)\\
\sigma &\sim \mathcal U(0.01, \inf) 
\end{align} $$


```{r linearregression2, message=FALSE, warning=FALSE, include=FALSE, cache=TRUE}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
modelstring2<-"
data {
  int N;
  real x[N];
  real y[N];
  real z[N];
}
parameters {
  real alpha;
  real beta;
  real omega;
  real sigma;
}
model {
  alpha ~ normal(0, 1);
  beta ~ cauchy(0,10);
  for(n in 1:N)
    y[n] ~ normal(x[n]*(beta+omega*z[n])+alpha, sigma);
}
generated quantities {
  real y_pred[N];
  for (n in 1:N)
    y_pred[n] = x[n]*(beta+omega*z[n])+alpha;
}
"
sfit2<-stan(model_code = modelstring2, data = list(N=nrow(mydf), x=mydf$x, y=mydf$y, z=mydf$zz))
sfitdf2<-as.data.frame(sfit2)
```

Comparing the results from our first model with the current one we can see that we're obviously doing better. The clustering works pretty well. The graph to the left is the first model and the one to the right is the revised model with an updated likelihood.

```{r 1vs2, echo=FALSE, message=FALSE, warning=FALSE}
multiplot(
  plotFit(sfitdf, mydf, 150) + theme_minimal() + scale_color_discrete() + xlab("x") + 
    ylab("y") + theme(legend.position = "none"),
  plotFit(sfitdf2, mydf, 150) + theme_minimal() + scale_color_discrete() + xlab("x") + 
    ylab("y") + theme(legend.position = "none"), cols = 2)
```

As is always instructional let's look at the posteriors of the parameters of our second model. They are depicted below. You can clearly see that the "increase in slope" parameter $\omega$ clearly captures the new behavior we wished to model.  

```{r model results, echo=FALSE, message=FALSE, warning=FALSE}
sfitdf2 %>% dplyr::select(-matches("y_pred")) %>% gather() %>% ggplot(aes(x=value)) + geom_histogram() + facet_wrap(~key, scales="free") + theme_minimal()
```

# Conclusion

This post has been about not becoming blind with respect to the mathematical restrictions we impose on the total model by sticking to a too simplistic representation. Also in this case the Bayesian formalism does not save us with it's bimodal capabilities since the model was misspecified.

- Think about all aspects of your model before you push the inference button
- Be aware that something that might appear as a clear cut case for multimodality may actually be a pathological problem in your model
- Also, be aware that sometimes multimodality *is* expected and totally ok

Happy inferencing!


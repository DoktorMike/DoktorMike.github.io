---
title: "On the apparent success of the maximum likelihood principle"
author: "Dr. Michael Green"
date: "Jul 26, 2017"
output: html_document
layout: post
published: false
status: process
use_math: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(rstan)
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(dautility)
library(damodel)
library(nloptr)
library(brms)
library(bayesplot)
rstan_options(auto_write = TRUE)
load("bayesVsFreqAgain.RData")
```

# Motivation

Today we will run through an important concept in statistical learning theory and modeling in general. It may come as no surprise that my point is as usual "age quod agis". This is a lifelong strive for me to convey that message to fellow scientists and business people alike. Anyway, back to the topic. We will have a look at why the Bayesian treatment of models is fundamentally important to everyone and not only a select few mathematically inclined experts. The model we will use for this post is a time series model

$$\begin{align}
y_t &\sim N(\mu_t, \sigma)\\
\mu_t &=\sum_{i=1}^{7}\beta_{i} x_{t,i} + \beta_0\\ 
\sigma &\sim U(0.01, \inf) 
\end{align}$$

which is a standard linear model. The $y_t$ is the observed sales units at time $t$ and the $x_{t,i}$ is the indicator variable for weekday $i$ at time $t$. As per usual $\beta_0$ serves as our intercept. A small sample of the data set looks like this  

```{r data, echo=FALSE, message=FALSE, warning=FALSE}
select(data, y, matches("WDay")) %>% head %>% knitr::kable()
```

which, for the response variable $y$ over time, looks like the plot below.

```{r problemplot, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Illustration of the time series we're trying to model."}
# mydf<-tibble(x=seq(0,30,0.2), y=x*ifelse(runif(1:length(x))>0.5, 1, 3)+rnorm(length(x), 0, 5))
ggplot(mydf, aes(y=newusers, x=date)) + geom_point() + theme_minimal() + scale_y_continuous(labels = comma) + ylab("Sales") + xlab("Date")
```

For those of you wth modeling experience you will see that a mere intra-weekly seasonality will not be enough for capturing all the interesting parts of this particular series but for the point I'm trying to make it will work just fine sticking with seasonality + and intercept. 

# Estimating the parameters of the model

We're going to estimate the parameters of this model by

1. The full Bayesian treatment, i.e., we're going to estimate $p(\beta|y, X)$
2. The Maximum likelihood, i.e., we're going to estimate $p(y|\beta, X)$

If you rememeber your probability theory training you know that $p(\beta|y, X) \neq p(y|\beta, X)$. Sure but so what? Well, this matters a lot. In order to see why let's dig into these terms. First off, let's have a look at the proper full Bayesian treatment. We can express that posterior distribution using three terms, namely the

1. **Likelihood**, 
2. the **Prior** and
3. the **Evidence**.

$$p(\beta|y, X)=\frac{p(y|\beta, X)p(\beta|X)}{\int p(y,\beta, X) d\beta}$$

The Evidence is the denominator and serves as a normalization factor that allows us to talk about probabilities in the first place. The nominator consists of two terms; the Likelihood (to the left), and the prior (to the right). It's worth noticing here that the prior for $\beta$ may very well depend on the covariates as such, and even on the response variable should we wish to venture into emperical priors. Explained in plain words the equation above states that we wish to estimate the posterior probability of our parameters $\beta$ by weigting our prior knowledge and assumptions about those parameters with the plausability of them generating a data set like ours, normalized by the plausability of the data itself under the existing mathematical model. Now doesn't that sound reasonable? I think it does.

Now if we look into the same kind of analysis for what the Maximum Likelihood method does we find the following equation

$$p(y|\beta, X)=\frac{p(\beta|y, X)}{p(\beta|X)}\int p(y,\beta, X) d\beta$$

which states that the probability of observing a data set like ours given fixed $\beta$'s is the posterior probability of the $\beta$'s divided by our prior assumptions scaled by the total plausability of the data itself. Now this also sounds reasonable, and it is. The only problem is that the quantity on the left hand side is not sampled; it is maximized in Maximum Likelihood. Hence the name.. On top of that what you do in 99% of all cases is ignore the right hand side in the equation above and just postulate that $p(y|\beta,X)=\mathcal{N}(\mu,\sigma)$ which is a rather rough statement to begin with, but let's not dive into that right now. So when you maximize this expression, what are you actually doing? Tadam! You're doing data fitting. This might seem like a good thing but it's not. Basically you're generating every conceivable hypothesis known to the model at hand and picking the one that happens to coincide the best with your, in most cases, tiny dataset. That's not even the worst part; The worst part is that you won't even, once the fitting is done, be able to express yourself about the uncertainty of the parameters of your model!

Now that we have skimmed through the surface of the math behind the two methodologies we're ready to look at some results and do the real analysis. 

## Technical setup

The Bayesian approach is estimated using the probabalistic programming language **Stan** following the model described in the beginning, i.e., we have uninformed priors. This is to make it as similar to the Maximum Likelihood method as possible. The Maximum Likelihood method is implemented using the *lm* function in **R**. Thus, in R we're simply doing

```{r, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
mylm <- lm(y~WDay1+WDay2+WDay3+WDay4+WDay5+WDay6+WDay7, data=ourdata)
```

meanwhile in Stan we're doing the following, admittedly a bit more complicated, code.

```{stan output.var="mugga", eval=FALSE}
data {
  int< lower = 0 > N;       // Number of data points
  vector[N] y;          // The response variable
  matrix[N, 7] xweekday; // The weekdays variables
}

parameters {
  real< lower = 0.01 > b0;  // The intercept
  vector[7 - 1] bweekday; // The weekday regression parameters
  real< lower = 0 > sigma;  // The standard deviation
}

transformed parameters {
  // Declarations
  vector[N] mu;
  vector[7] bweekdayhat;

  // The weekday part
  bweekdayhat[1] = 0;
  for (i in 1:(7 - 1) ) bweekdayhat[i + 1] = bweekday[i];
  
  // The mean prediction each timestep
  mu = b0 + xweekday*bweekdayhat;
}

model {
  // Priors
  b0 ~ normal(mean(ynew), sd(ynew));
  bweekday ~ normal(0, sd(ynew));

  // Likelihood
  y ~ normal(mu, sigma);
}

generated quantities {
  vector[N] yhat;
  yhat = b0 + xweekday * bweekdayhat;
}
```

If you're not in the mood to learn Stan you can achieve the same thing by using the **brms** package in R and run the following code

```{r, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
require(brms)
mybrms <- brm(bf(y~WDay1+WDay2+WDay3+WDay4+WDay5+WDay6+WDay7), data=ourdata, cores = 2, chains = 4)
```

which will write, compile and sample your model in Stan and return it to R.

# Results

Now to the dirty details of our calculations for the parameter estimates of the model.

```{r Estimatesummaries, echo=FALSE, message=FALSE, warning=FALSE}
a<-summary(mybrms)
a<-a$fixed[,1:2]
# lm version
b<-summary(mylm2)
b<-rbind(b$coefficients[,1:2], NA) %>% as.data.frame()
rownames(b)<-c("(Intercept)", "WDay1", "WDay2", "WDay3", "WDay4", "WDay5", 
"WDay6", "WDay7")
knitr::kable(cbind(a,b), digits = c(0,0,0,0), caption = "Table: Coefficients from sampling the posterior probability on the left hand side of the table and from maximum likelihood on the right hand side. Notice the NA in the estimation using the ML method.")
```

If you're looking at the table above, you might think "What the damn hell!?", Bayesian statistics makes no sense at all! Why did we get these crazy estimates? Look at the nice narrow **confidence** intervals on the right hand side of the table generated by the maximum likelihood estimates and compare them to the wide **credibility** intervals to the left. You might be forgiven for dismissing the results from the Bayesian approach, since the difference is quite subtle from a mathematical point of view. After all we are computing the exact same mathematical model. The difference is our reasoning about the parameters. If you remember correctly maximum likelihood views the parameters as fixed constants without any variation. The variation you see in maximum likelihood comes from the uncertainty about the data and not the parameters! This is important to remember. The "Std. Error" from the maximum likelihood estimate has nothing to do with uncertainty about the parameter values for the observed data set. Instead it's uncertainty regarding what would happen to the estimates if we observed more data sets that looks like ours. Statistically speaking what ML does is maximize

$$p(y|\beta,X)$$
which expresses likelihood over different $y$'s given an observed and fixed set of parameters $\beta$ along with covariates $X$.

# Model validation and sanity checking

```{r posteriorsbeta, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10}
myps<-posterior_samples(mybrms, add_chain = T)
mypp<-posterior_predict(mybrms)
mcmc_dens_overlay(myps, regex_pars = "b_WD") + vline_at(tail(b[,1], nrow(b)-1), linetype = 1, size = 1)
```

```{r posteriorpredict, echo=FALSE, message=FALSE, warning=FALSE}
ppc_dens_overlay(mydf$newusers, mypp[1:35,])
```


```{r performance, echo=FALSE, message=FALSE, warning=FALSE}
losses <- function(p, o) c(MAE=mae(p, o), MASE=mase(p, o), MAPE=mape(p, o),
                           RMSE=rmse(p, o), NRMSE=nrmse(p, o), CVRMSE=cvrmse(p, o),
                           R2=r2(p, o))
tmpdf <- data.frame(Bayes=round(losses(preddf$Bayes, preddf$Observed), 2), 
       Freq=round(losses(preddf$Freq, preddf$Observed),2))
knitr::kable(tmpdf)
```

Happy inferencing!


---
title: "A first look at Edward!"
author: "Dr. Michael Green"
date: "Feb 19, 2017"
output: html_document
layout: post
published: false
status: process
use_math: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(rstan)
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(dautility)
library(damodel)
library(nloptr)
rstan_options(auto_write = TRUE)
```

# Motivation

There's a new kid on the inference block called "Edward" who is full of potential and promises of merging probabilistic programming, computational graphs and inference! There's also talk of a 35 times speed up compared to our good old reliable fellow "Stan". Today I will run some comparisons for problems that currently interest me namely time series with structural hyperparameters.

To start things off and make sure we have all our ducks in a row for running edward we need to install it using the python installer called pip which is available in most linux distros. I will use pip3 here because I use python3 instead of python2. It shouldn't matter which one you choose though. So go ahead and install "Edward".

```{bash installedward, eval=FALSE, include=TRUE}
sudo pip3 install edward
```

If you ran this in the console you should now have a working version of Edward installed in your python environment. So far so good. Just to make sure it works we will run a small Bayesian Neural Network with Gaussian priors for the weights. This is the standard example from Edwards web page. It uses a Variational Inference approach to turn the sampling problem into an optimization problem by approximating the target posterior by a multivariate Gaussian. This approach works ok for quite a few problems. However, it is a tad hyped as a general purpose inference sampler and should not be considered as a replacement for a real sampler. In either case try to run the code below and check it out. For this toy dataset it works fine. ;) 

```{r engine='python', engine.path='python3', cache=FALSE, eval=FALSE}
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import edward as ed
import numpy as np
import tensorflow as tf

from edward.models import Normal

def build_toy_dataset(N=50, noise_std=0.1):
  x = np.linspace(-3, 3, num=N)
  y = np.cos(x) + np.random.normal(0, noise_std, size=N)
  x = x.astype(np.float32).reshape((N, 1))
  y = y.astype(np.float32)
  return x, y

def neural_network(x, W_0, W_1, b_0, b_1):
  h = tf.tanh(tf.matmul(x, W_0) + b_0)
  h = tf.matmul(h, W_1) + b_1
  return tf.reshape(h, [-1])


ed.set_seed(42)

N = 50  # number of data ponts
D = 1   # number of features

# DATA
x_train, y_train = build_toy_dataset(N)

# MODEL
W_0 = Normal(mu=tf.zeros([D, 2]), sigma=tf.ones([D, 2]))
W_1 = Normal(mu=tf.zeros([2, 1]), sigma=tf.ones([2, 1]))
b_0 = Normal(mu=tf.zeros(2), sigma=tf.ones(2))
b_1 = Normal(mu=tf.zeros(1), sigma=tf.ones(1))

x = x_train
y = Normal(mu=neural_network(x, W_0, W_1, b_0, b_1),
           sigma=0.1 * tf.ones(N))

# INFERENCE
qW_0 = Normal(mu=tf.Variable(tf.random_normal([D, 2])),
              sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D, 2]))))
qW_1 = Normal(mu=tf.Variable(tf.random_normal([2, 1])),
              sigma=tf.nn.softplus(tf.Variable(tf.random_normal([2, 1]))))
qb_0 = Normal(mu=tf.Variable(tf.random_normal([2])),
              sigma=tf.nn.softplus(tf.Variable(tf.random_normal([2]))))
qb_1 = Normal(mu=tf.Variable(tf.random_normal([1])),
              sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))

inference = ed.KLqp({W_0: qW_0, b_0: qb_0,
                     W_1: qW_1, b_1: qb_1}, data={y: y_train})


# Sample functions from variational model to visualize fits.
rs = np.random.RandomState(0)
inputs = np.linspace(-5, 5, num=400, dtype=np.float32)
x = tf.expand_dims(tf.constant(inputs), 1)
mus = []
for s in range(10):
  mus += [neural_network(x, qW_0.sample(), qW_1.sample(),
                         qb_0.sample(), qb_1.sample())]

mus = tf.stack(mus)

sess = ed.get_session()
init = tf.global_variables_initializer()
init.run()

import pandas as pd

# Prior samples
outputs = mus.eval()
mydf=pd.DataFrame(outputs)
mydf.to_csv("mydfprior.csv")

# Inference
inference.run(n_iter=500, n_samples=5)

# Posterior samples
outputs = mus.eval()
mydf=pd.DataFrame(outputs)
mydf.to_csv("mydfpost.csv")

```

## Checking the priors

If you ran the code you will now have two files called mydfprior.csv and mydfpost.csv which contains, surprise, surprise, your prior and posterior curves based on the samples. As always we plot out the consequence of our priors and check what happens. In the plot below you can see that our priors for the Bayesian Neural Network do not really produce curves that resembles what we're looking for. Not to worry my friends; we don't actually need them to. Look through the graph and make sure you understand why the plot looks the way it does. 

```{r Prior plot, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyr)
library(dplyr)

# Read data from Python code
priordf <- read.csv("mydfprior.csv")[,-1]
postdf <- read.csv("mydfpost.csv")[,-1]

priordf %>% mutate(s=1:10) %>% gather(x, y, -s) %>% mutate(x=as.numeric(gsub("X", "", x)), s=as.factor(s)) %>% ggplot(aes(y=y, x=x, group=s)) + geom_point(color="magenta", alpha=0.5) + theme_minimal() + geom_point(aes(x=x, y=y, group=0), data=tibble(z=seq(-4,4,length.out = 400), x=1:length(z), y=cos(z)+rnorm(length(z), 0, sqrt(0.1))))
```

## Checking the posteriors

> The problem with the world is not that people know too little. It's that they know so many things that just ain't so.
-- Mark Twain

So the priors hopefully makes sense to you now. How about our posterior? Well as you can see below this plot makes a lot more sense compared to the data we're trying to make sense of. However, you can clearly see some uncertainty in there as well. This is key, since no matter which model we choose there will always be elements of uncertainty involved. The great thing about science is that we don't have to pretend to know everything. We're perfectly comfortable admitting our ignorence. The probabilistic framework allows us to quantify that ignorance! If you think that sounds like a bad idea I suggest you take a look at the quote above. 

```{r Post plot, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyr)
library(dplyr)

# Read data from Python code
priordf <- read.csv("mydfprior.csv")[,-1]
postdf <- read.csv("mydfpost.csv")[,-1]

postdf %>% mutate(s=1:10) %>% gather(x, y, -s) %>% mutate(x=as.numeric(gsub("X", "", x)), s=as.factor(s)) %>% ggplot(aes(y=y, x=x, group=s)) + geom_point(color="magenta", alpha=0.5) + theme_minimal() + geom_point(aes(x=x, y=y, group=0), data=tibble(z=seq(-4,4,length.out = 400), x=1:length(z), y=cos(z)+rnorm(length(z), 0, sqrt(0.1))))
```

# A more real world problem

There are few real world problems as pressing as that of global warming. Whenever I'm talking about global warming I feel there are two responses I get which are basically binomially distributed in which the majority of people quickly gets it and the other part are oblivious to facts presented to them. In reality there can be little to no doubt that humans are causing the green house effect. The plot below shows the evolution of the temperature anomaly over time since the 1850's until present time.

```{r globwarmdata, echo=FALSE, message=FALSE, warning=FALSE}
fnameglobtemp<-"http://www.metoffice.gov.uk/hadobs/hadcrut4/data/current/time_series/HadCRUT.4.5.0.0.monthly_ns_avg.txt"
tempdf <- read.delim(file = fnameglobtemp, header = F, sep="")
tempdf <- tempdf[,1:2]; colnames(tempdf)<-c("date", "tempdev")
tempdf$date <- lubridate::ymd(paste0(as.character(tempdf$date), "/01"))
ggplot(tempdf, aes(y=tempdev, x=date)) + geom_point() + geom_smooth() + ylab("Temperature deviance") + xlab("Date") + theme_minimal()
```

As you see there is a clear trend showing that the world is getting warmer. So global warming is indeed happening and it's causing some very measurable real problems for us. There are lobbyist organizations around the world who wishes to tell you that this is not really caused by our increased CO2 emissions since they have well ulterior motives. 

```{r globflightsdata, echo=FALSE, message=FALSE, warning=FALSE}
flightdf<-read.csv("tran_r_avpa_nm_1_Data.csv")
flightdf$Value<-as.numeric(gsub(",|:", "", as.character(flightdf$Value)))
flightdf %>% group_by(Year=TIME) %>% summarise(Passengers=sum(Value, na.rm = T)) %>% ggplot(aes(y=Passengers, x=Year)) + geom_point() + theme_minimal()
```


```{r}
adf<-tempdf %>% dplyr::filter(date >= "1993-01-01", date < "2016-01-01") %>% group_by(Year=year(date)) %>% summarise(Tempdev=mean(tempdev))
bdf<-flightdf %>% group_by(Year=TIME) %>% summarise(Passengers=sum(Value, na.rm = T))
mydf<-inner_join(adf, bdf)
mydf %>% gather(Metric, Value, -Year) %>% ggplot(aes(y=Value, x=Year, color=Metric)) + facet_grid(Metric~., scales="free_y") + geom_point() + theme_minimal() + geom_smooth(method = lm)
```


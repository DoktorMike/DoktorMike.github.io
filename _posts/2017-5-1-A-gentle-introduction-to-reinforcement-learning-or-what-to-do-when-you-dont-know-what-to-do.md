---
title: "A gentle introduction to reinforcement learning or what to do when you don't know what to do"
author: "Dr. Michael Green"
date: "May 1, 2017"
output: html_document
layout: post
published: true
status: publish
use_math: true
---


# Introduction
Today we're going to have a look at an interesting set of learning algorithms which does not require you to know the truth while you learn. As such this is a mix of unsupervised and supervised learning. The supervised part comes from the fact that you look in the rear view mirror after the actions have been taken and then adapt yourself based on how well you did. This is surprisingly powerful as it can learn whatever the knowledge representation allows it to. One caveat though is that it is excruciatingly sloooooow. This naturally stems from the fact that there is no concept of a right solution. Neither when you are making decisions nor when you are evaluating them. All you can say is that "Hey, that wasn't so bad given what I tried before" but you cannot say that it was the best thing to do. This puts a dampener on the learning rate. The gain is that we can learn just about anything given that we can observe the consequence of our actions in the environment we operate in.

![plot of the reinforcement learning loop](/images/figure/reinforcement.png)

As illustrated above, reinforcement learning can be thought of as an agent acting in an environment and receiving rewards as a consequence of those actions. This is in principle a Markov Decision Process (MDP) which basically captures just about anything you might want to learn in an environment. Formally the MDP consists of

* A set of states $[s_1, s_2, ..., s_M]$
* A set of actions $[a_1, a_2, ..., a_N]$
* A set of rewards $[r_1, r_2, ..., r_L]$
* A set of transition probabilities $[s_{11}, s_{12}, ..., s_{1M}, s_{21}, s_{22}, ..., s_{2M}, ..., s_{MM}]$

which looks surprisingly simple but is really all we need. The mission is to learn the best transition probabilities that maximizes the expected total future reward. Thus to move on we need to introduce a little mathematical notation. First off we need a reward function $R(s_t, a_t)$ which gives us the reward $r_t$ that comes from taking action $a_t$ in state $s_t$ at time $t$. We also need a transition function $S(s_t, a_t)$ which will give us the next state $s_{t+1}$. The actions $a_t$ are generated by the agent by following one or several policies. A policy function $P(s_t)$ therefore generates an action $a_t$ which will, to it's knowledge, give the maximum reward in the future.

# The problem we will solve - Cart Pole
We will utilize an environment from the AI Gym called the Cart pole problem. The task is basically learning how to balance a pole by controlling a cart. The environment gives us a new state every time we act in it. This state consists of four observables corresponding to position and movements. Before showing you the implementation we'll have a look at how a trained agent performs below.

![plot of a working solution](/images/figure/gymcartpolesolved.gif)

As you can see it performs quite well and actually manages to balance the pole by controlling the cart in real time. You might think that hey that sounds easy I'll just generate random actions and it should cancel out. Well, put your mind at ease. Below you can see an illustration of that approach failing.

![plot of a working solution](/images/figure/gymcartpolenotsolved.gif)

So to the problem at hand. How can we model this? We need to make an agent that learns a policy that maximizes the future reward right? Right, so at any given time our policy can choose one of two possible actions namely

1. move left
2. move right

which should sound familiar to you if you've done any modeling before. This is basically a Bernoulli model where the probability distribution looks like this $P(y;p)=p^y(1-p)^{1-y}$. Once we know this the task is to model $p$ as a function of the current state $s_t$. This can be done by doing a linear model wrapped by a sigmoid like this

$$p_t=P(s_t; \omega)=\frac{1}{1+\exp(-\omega s_t)}$$

where $\omega$ are the four parameters that will basically control which way we want to move. These four parameters makes up the policy. With these two pieces we can set up a likelihood function that can drive our learning.

$$L(\omega, s, y)=\prod_{t=1}^T p_t^{y_t}(1-p_t)^{1-y_t} $$

where $p_t$ is defined above. This likelihood we want to maximize and in order to do that we will turn in around and instead minimize the negative log likelihood

$$l(\omega)=-\ln L(\omega, s, y)=-\sum_{t=1}^T \left(y_t \ln p_t + (1-y_t) \ln (1-p_t) \right) $$

$$\frac{\partial l(\omega)}{\partial \omega}=0$$

$$\delta\omega=-\eta\frac{\partial l(\omega)}{\partial \omega}$$

# Implementation

## The python functions you're going to need
As we're implementing this in python3 and it's not always straightforward what is python3 and python2 I'm sharing the function definitions with you that I created since they are indeed compliant with the python3 libraries. Especially Numpy which is an integral part of computation in Python. Most of these functions are easily implemented and understood, but they're added here for completeness.

```python
def discount_rewards(r, gamma=1-0.99):
    df = np.zeros_like(r)
    for t in range(len(r)):
        df[t] = np.npv(gamma, r[t:len(r)])
    return df

def sigmoid(x):
    return 1.0/(1.0+np.exp(-x))

def dsigmoid(x):
    a=sigmoid(x)
    return a*(1-a)

def decide(b, x):
    return sigmoid(np.vdot(b, x))

def loglikelihood(y, p):
    return y*np.log(p)+(1-y)*np.log(1-p)

def weighted_loglikelihood(y, p, dr):
    return (y*np.log(p)+(1-y)*np.log(1-p))*dr

def loss(y, p, dr):
	return -weighted_loglikelihood(y, p, dr)

def dloss(y, p, dr, x):
    return np.reshape(dr*( (1-np.array(y))*p - y*(1-np.array(p))), [len(y),1])*x
```

# Multiple solutions


![plot of all possible solutions](/images/figure/solutiondistribution.png)


# Conclusion

Happy inferencing!
